{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fJTQ6-lxfsvj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_ibBTNXIf0xv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"class CaptchaDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, img_width=200, img_height=50, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create character to index mapping\n",
    "        # Start from index 1, leave 0 for CTC blank\n",
    "        self.characters = sorted(list(set(char for label in labels for char in label)))\n",
    "        print(self.characters)\n",
    "        self.char_to_idx = {char: idx + 1 for idx, char in enumerate(self.characters)}\n",
    "        self.idx_to_char = {idx + 1: char for idx, char in enumerate(self.characters)}\n",
    "        # Add blank token\n",
    "        self.idx_to_char[0] = ''\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        label_indices = [self.char_to_idx[char] for char in label]\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': torch.tensor(label_indices, dtype=torch.long),\n",
    "            'label_length': torch.tensor(len(label_indices), dtype=torch.long),\n",
    "            'text': label\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\"\"\"\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, img_width=200, img_height=50, transform=None,\n",
    "                 char_to_idx=None, idx_to_char=None):\n",
    "        self.image_paths = image_paths\n",
    "        # Filter out non-alphanumeric characters from labels\n",
    "        self.labels = [''.join(char for char in label if char.isalnum()) for label in labels]\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.transform = transform\n",
    "        \n",
    "        if char_to_idx is None or idx_to_char is None:\n",
    "            # Only create mappings if not provided\n",
    "            self.characters = sorted(list(set(char for label in self.labels for char in label)))\n",
    "            self.char_to_idx = {char: idx + 1 for idx, char in enumerate(self.characters)}\n",
    "            self.idx_to_char = {idx + 1: char for idx, char in enumerate(self.characters)}\n",
    "            self.idx_to_char[0] = ''\n",
    "        else:\n",
    "            self.char_to_idx = char_to_idx\n",
    "            self.idx_to_char = idx_to_char\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        label_indices = [self.char_to_idx[char] for char in label]\n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': torch.tensor(label_indices, dtype=torch.long),\n",
    "            'label_length': torch.tensor(len(label_indices), dtype=torch.long),\n",
    "            'text': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cY5brQ6_f70y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OCRModel(nn.Module):\n",
    "    def __init__(self, num_chars, img_width=200, img_height=50):\n",
    "        super(OCRModel, self).__init__()\n",
    "\n",
    "        # CNN layers with batch normalization\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Calculate features shape after CNN\n",
    "        feature_h = img_height // 4\n",
    "        feature_w = img_width // 4\n",
    "\n",
    "        # Dense layer after CNN\n",
    "        self.dense1 = nn.Sequential(\n",
    "            nn.Linear(feature_h * 64, 128),\n",
    "            nn.BatchNorm1d(50),  # Add batch norm here too\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Bidirectional LSTM layers\n",
    "        self.lstm1 = nn.LSTM(128, 128, bidirectional=True, batch_first=True, dropout=0.25)\n",
    "        self.lstm2 = nn.LSTM(256, 64, bidirectional=True, batch_first=True, dropout=0.25)\n",
    "\n",
    "        # Final dense layer\n",
    "        self.dense2 = nn.Linear(128, num_chars + 1)  # +1 for CTC blank\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        x = self.cnn(x)\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # Reshape for dense layer\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = x.contiguous().view(batch_size, width, height * channels)\n",
    "\n",
    "        # Dense layer\n",
    "        x = self.dense1(x)\n",
    "\n",
    "        # LSTM layers\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "\n",
    "        # Final dense layer\n",
    "        x = self.dense2(x)\n",
    "        x = nn.functional.log_softmax(x, dim=2)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SzvJ8WhTgBQc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_batch(model, batch, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    images = batch['image'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "    label_lengths = batch['label_length'].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    log_probs = model(images)\n",
    "    batch_size = images.size(0)\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=log_probs.size(1), dtype=torch.long).to(device)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(log_probs.transpose(0, 1), labels, input_lengths, label_lengths)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SNoHDzMCgDv4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_predictions(log_probs, idx_to_char):\n",
    "    \"\"\"Decode CTC output to text\"\"\"\n",
    "    pred_indices = torch.argmax(log_probs, dim=2)\n",
    "    batch_texts = []\n",
    "\n",
    "    for pred in pred_indices:\n",
    "        text = []\n",
    "        # Remove duplicate indices and CTC blank label\n",
    "        for i in range(len(pred)):\n",
    "            if i == 0 or pred[i] != pred[i-1]:\n",
    "                if pred[i] != 0:  # 0 is assumed to be CTC blank\n",
    "                    text.append(idx_to_char[pred[i].item()])\n",
    "        batch_texts.append(''.join(text))\n",
    "\n",
    "    return batch_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Km-iLi76gJU1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=100, device='cuda'):\n",
    "    criterion = nn.CTCLoss(zero_infinity=True, blank=0)  # Set blank index to 0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = early_stopping_patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            label_lengths = batch['label_length'].to(device)\n",
    "\n",
    "            log_probs = model(images)\n",
    "            batch_size = images.size(0)\n",
    "            input_lengths = torch.full(size=(batch_size,), fill_value=log_probs.size(1), dtype=torch.long).to(device)\n",
    "\n",
    "            loss = criterion(log_probs.transpose(0, 1), labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                label_lengths = batch['label_length'].to(device)\n",
    "\n",
    "                log_probs = model(images)\n",
    "                batch_size = images.size(0)\n",
    "                input_lengths = torch.full(size=(batch_size,), fill_value=log_probs.size(1), dtype=torch.long).to(device)\n",
    "\n",
    "                loss = criterion(log_probs.transpose(0, 1), labels, input_lengths, label_lengths)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vR_1EWNbgSM3",
    "outputId": "9da1ffcf-094f-4bec-bb53-6e12c3da1bf5"
   },
   "source": [
    "!curl -LO https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip\n",
    "!unzip -qq captcha_images_v2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaR4TK-0gaDr",
    "outputId": "6cdfe311-b06f-4bdc-c5ce-580502457da9"
   },
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"resppony/6-digit-alphanumeric-captcha-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q9JfS2EshuCL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable length sequences\n",
    "    Args:\n",
    "        batch: List of dictionaries containing 'image', 'label', 'label_length', and 'text'\n",
    "    \"\"\"\n",
    "    # Sort batch by label length in descending order\n",
    "    batch.sort(key=lambda x: len(x['label']), reverse=True)\n",
    "    \n",
    "    # Get maximum sequence length in this batch\n",
    "    max_length = len(batch[0]['label'])\n",
    "    \n",
    "    # Prepare lists for batch items\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_lengths = []\n",
    "    texts = []\n",
    "    \n",
    "    for item in batch:\n",
    "        images.append(item['image'])\n",
    "        # Pad labels to max_length\n",
    "        curr_label = item['label']\n",
    "        curr_len = len(curr_label)\n",
    "        if curr_len < max_length:\n",
    "            # Pad with zeros (blank token)\n",
    "            padding = torch.zeros(max_length - curr_len, dtype=torch.long)\n",
    "            curr_label = torch.cat([curr_label, padding])\n",
    "        labels.append(curr_label)\n",
    "        label_lengths.append(item['label_length'])\n",
    "        texts.append(item['text'])\n",
    "    \n",
    "    # Stack all tensors\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "    label_lengths = torch.stack(label_lengths)\n",
    "    \n",
    "    return {\n",
    "        'image': images,\n",
    "        'label': labels,\n",
    "        'label_length': label_lengths,\n",
    "        'text': texts\n",
    "    }\n",
    "\n",
    "def create_datasets(images, labels, batch_size=16, train_size=0.9, transform=None):\n",
    "    \"\"\"\n",
    "    Create datasets with custom collate function\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    indices = torch.randperm(len(images))\n",
    "    train_size = int(len(images) * train_size)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "\n",
    "    # If no transform provided, create default transform\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((50, 200)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CaptchaDataset(\n",
    "        [images[i] for i in train_indices],\n",
    "        [labels[i] for i in train_indices],\n",
    "        transform=transform,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char\n",
    "    )\n",
    "    \n",
    "    val_dataset = CaptchaDataset(\n",
    "        [images[i] for i in val_indices],\n",
    "        [labels[i] for i in val_indices],\n",
    "        transform=transform,\n",
    "        char_to_idx=char_to_idx,\n",
    "        idx_to_char=idx_to_char\n",
    "    )\n",
    "\n",
    "    # Create dataloaders with custom collate function\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, train_dataset.char_to_idx, train_dataset.idx_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uoaGurNViY9S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define expanded character set (A-Z, a-z, 0-9)\n",
    "characters = (\n",
    "    [str(i) for i in range(10)] +  # 0-9\n",
    "    [chr(i) for i in range(65, 91)] +  # A-Z\n",
    "    [chr(i) for i in range(97, 123)]   # a-z\n",
    ")\n",
    "characters = sorted(characters)  # Sort to ensure consistent ordering\n",
    "\n",
    "# Create character mappings\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(characters)}\n",
    "idx_to_char = {idx + 1: char for idx, char in enumerate(characters)}\n",
    "# Add blank token\n",
    "idx_to_char[0] = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sQ_op24WhHcZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def combine_datasets(old_data_dir, new_data_dir, batch_size=16, train_size=0.9):\n",
    "#     # Load original dataset paths and labels\n",
    "#     old_images = sorted(list(map(str, list(Path(old_data_dir).glob(\"*.png\")))))\n",
    "#     old_labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in old_images]\n",
    "\n",
    "#     # Load new dataset paths and labels\n",
    "#     new_images = sorted(list(map(str, list(Path(new_data_dir).glob(\"*.png\")))))\n",
    "#     new_labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in new_images]\n",
    "\n",
    "#     # print(f\"Number of images found in new_data_dir: {len(new_images)}\")\n",
    "#     # print(f\"New data directory path: {new_data_dir}\")\n",
    "    \n",
    "#     # Combine datasets\n",
    "#     #all_images = old_images + new_images\n",
    "#     #all_labels = old_labels + new_labels\n",
    "\n",
    "#     # Create datasets with combined data\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((50, 200)),\n",
    "#         transforms.ToTensor(),\n",
    "#     ])\n",
    "\n",
    "#     return create_datasets(new_images, new_labels, batch_size, train_size, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# jpg png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_datasets(old_data_dir, new_data_dir, batch_size=16, train_size=0.9):\n",
    "    # Load old dataset paths and labels (both .png and .jpg)\n",
    "    old_images_png = list(Path(old_data_dir).glob(\"*.png\"))\n",
    "    old_images_jpg = list(Path(old_data_dir).glob(\"*.jpg\"))\n",
    "    old_images = sorted(list(map(str, old_images_png + old_images_jpg)))\n",
    "    old_labels = [img.split(os.path.sep)[-1].split(\".\")[0] for img in old_images]\n",
    "\n",
    "    # Load new dataset paths and labels (both .png and .jpg)\n",
    "    new_images_png = list(Path(new_data_dir).glob(\"*.png\"))\n",
    "    new_images_jpg = list(Path(new_data_dir).glob(\"*.jpg\"))\n",
    "    new_images = sorted(list(map(str, new_images_png + new_images_jpg)))\n",
    "    new_labels = [img.split(os.path.sep)[-1].split(\".\")[0] for img in new_images]\n",
    "\n",
    "    # Combine datasets\n",
    "    all_images = old_images + new_images\n",
    "    all_labels = old_labels + new_labels\n",
    "\n",
    "    # Transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((50, 200)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    return create_datasets(all_images, all_labels, batch_size, train_size, transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PbGevcnhhzFn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"def update_model_for_new_chars(model_path, new_num_chars):\n",
    "    model = OCRModel(num_chars=len(char_to_idx))\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Create new final layer with expanded character set\n",
    "    old_dense2 = model.dense2\n",
    "    model.dense2 = nn.Linear(128, new_num_chars + 1)  # +1 for CTC blank\n",
    "\n",
    "    # Copy over existing weights for previously known characters\n",
    "    with torch.no_grad():\n",
    "        model.dense2.weight[:old_dense2.weight.shape[0]] = old_dense2.weight\n",
    "        model.dense2.bias[:old_dense2.bias.shape[0]] = old_dense2.bias\n",
    "\n",
    "    return model\n",
    "    \"\"\"\n",
    "def update_model_for_new_chars(model_path, char_to_idx):\n",
    "    # First, load the old model with original number of characters\n",
    "    num_char = len(char_to_idx) + 1\n",
    "    print(num_char)\n",
    "    old_model = OCRModel(num_chars=63)  # Original 19 chars (the +1 for blank is handled in the model)\n",
    "    old_model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Create new model with expanded character set\n",
    "    new_model = OCRModel(num_chars=num_char)  # 62 chars (the +1 for blank is handled in the model)\n",
    "\n",
    "    # Copy all layers except the last dense layer\n",
    "    new_model.cnn.load_state_dict(old_model.cnn.state_dict())\n",
    "    new_model.dense1.load_state_dict(old_model.dense1.state_dict())\n",
    "    new_model.lstm1.load_state_dict(old_model.lstm1.state_dict())\n",
    "    new_model.lstm2.load_state_dict(old_model.lstm2.state_dict())\n",
    "\n",
    "    # Copy weights for known characters in the final layer\n",
    "    with torch.no_grad():\n",
    "        new_model.dense2.weight[:64, :] = old_model.dense2.weight  # Copy old weights (including blank)\n",
    "        new_model.dense2.bias[:64] = old_model.dense2.bias\n",
    "\n",
    "        # Initialize the weights for new characters with small random values\n",
    "        nn.init.kaiming_normal_(new_model.dense2.weight[64:, :], mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(new_model.dense2.bias[64:], 0)\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "id": "vGjGEgGBh1zz",
    "outputId": "0b1d69e4-f2ca-4fa9-fa78-a4952e0b330e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "Epoch 1/30:\n",
      "Train Loss: 1.4144, Val Loss: 0.7780\n",
      "Epoch 2/30:\n",
      "Train Loss: 0.6597, Val Loss: 0.5203\n",
      "Epoch 3/30:\n",
      "Train Loss: 0.4751, Val Loss: 0.4015\n",
      "Epoch 4/30:\n",
      "Train Loss: 0.3626, Val Loss: 0.3355\n",
      "Epoch 5/30:\n",
      "Train Loss: 0.2917, Val Loss: 0.2830\n",
      "Epoch 6/30:\n",
      "Train Loss: 0.2437, Val Loss: 0.2516\n",
      "Epoch 7/30:\n",
      "Train Loss: 0.2080, Val Loss: 0.2271\n",
      "Epoch 8/30:\n",
      "Train Loss: 0.1774, Val Loss: 0.2105\n",
      "Epoch 9/30:\n",
      "Train Loss: 0.1546, Val Loss: 0.2021\n",
      "Epoch 10/30:\n",
      "Train Loss: 0.1307, Val Loss: 0.1812\n",
      "Epoch 11/30:\n",
      "Train Loss: 0.1185, Val Loss: 0.1896\n",
      "Epoch 12/30:\n",
      "Train Loss: 0.1057, Val Loss: 0.1647\n",
      "Epoch 13/30:\n",
      "Train Loss: 0.0932, Val Loss: 0.1725\n",
      "Epoch 14/30:\n",
      "Train Loss: 0.0777, Val Loss: 0.1742\n",
      "Epoch 15/30:\n",
      "Train Loss: 0.0720, Val Loss: 0.1644\n",
      "Epoch 16/30:\n",
      "Train Loss: 0.0671, Val Loss: 0.1695\n",
      "Epoch 17/30:\n",
      "Train Loss: 0.0611, Val Loss: 0.1536\n",
      "Epoch 18/30:\n",
      "Train Loss: 0.0533, Val Loss: 0.1361\n",
      "Epoch 19/30:\n",
      "Train Loss: 0.0457, Val Loss: 0.1428\n",
      "Epoch 20/30:\n",
      "Train Loss: 0.0442, Val Loss: 0.1405\n",
      "Epoch 21/30:\n",
      "Train Loss: 0.0390, Val Loss: 0.1397\n",
      "Epoch 22/30:\n",
      "Train Loss: 0.0339, Val Loss: 0.1935\n",
      "Epoch 23/30:\n",
      "Train Loss: 0.0342, Val Loss: 0.1307\n",
      "Epoch 24/30:\n",
      "Train Loss: 0.0314, Val Loss: 0.1354\n",
      "Epoch 25/30:\n",
      "Train Loss: 0.0306, Val Loss: 0.1446\n",
      "Epoch 26/30:\n",
      "Train Loss: 0.0261, Val Loss: 0.1383\n",
      "Epoch 27/30:\n",
      "Train Loss: 0.0229, Val Loss: 0.1391\n",
      "Epoch 28/30:\n",
      "Train Loss: 0.0234, Val Loss: 0.1385\n",
      "Epoch 29/30:\n",
      "Train Loss: 0.0220, Val Loss: 0.1473\n",
      "Epoch 30/30:\n",
      "Train Loss: 0.0172, Val Loss: 0.1248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Set device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Load and combine datasets\n",
    "# train_loader, val_loader, char_to_idx, idx_to_char = combine_datasets(\n",
    "#     old_data_dir=\"./captcha_images_v2/\",\n",
    "#     new_data_dir=\"./datasets/combined_labeled/\",\n",
    "#     batch_size=16\n",
    "# )\n",
    "\n",
    "# print(type(char_to_idx))\n",
    "# # Update model with new character set\n",
    "# model = update_model_for_new_chars('best_model.pth', char_to_idx)  # +1 for blank token\n",
    "# model = model.to(device)\n",
    "# # Continue training/content/best_model.pth\n",
    "# train_model(\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     num_epochs=30,  # Fewer epochs since model is pre-trained\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load and combine datasets\n",
    "train_loader, val_loader, char_to_idx, idx_to_char = combine_datasets(\n",
    "    old_data_dir=\"./captcha_images_v2/\",\n",
    "    new_data_dir=\"./datasets/combined_labeled/\",\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Update model with new character set and train\n",
    "model = update_model_for_new_chars('best_model.pth', char_to_idx)\n",
    "model = model.to(\"cuda\")\n",
    "train_model(model, train_loader, val_loader, num_epochs=30, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSFg5Fkm5HIn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHjeGAn048Sh"
   },
   "source": [
    "# After training, single image inference\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lDm1Z8Msr16O",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_batch_predictions(model, images, idx_to_char, device='cuda'):\n",
    "    \"\"\"Predict and decode the images to text\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        log_probs = model(images)\n",
    "\n",
    "    pred_texts = decode_predictions(log_probs, idx_to_char)\n",
    "    return pred_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lts73-FTrw1d",
    "outputId": "988d7ed1-ea70-46a0-86f8-37b4915c7768",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: 008549\n"
     ]
    }
   ],
   "source": [
    "def predict_single_image(image_path, model, transform, idx_to_char, device='cuda'):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Get prediction\n",
    "    pred_text = decode_batch_predictions(model, image, idx_to_char, device)[0]\n",
    "    return pred_text\n",
    "\n",
    "# Example usage for single image prediction:\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((50, 200)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "image_path = \"./dataset2/validation/validation/008549.babde59a74748445130ddd0732a44dff.jpg\" # Replace with your image path\n",
    "prediction = predict_single_image(image_path, model, transform, idx_to_char, device)\n",
    "print(f\"Predicted text: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'full_captcha_model.pth')\n",
    "\n",
    "# Or save just the model state dictionary (recommended)\n",
    "torch.save(model.state_dict(), 'captcha_model_state.pth')\n",
    "\n",
    "torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
